{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ruEkBUtuEzGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JvMRbVLEJlZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e41d44-57bc-41c2-86f7-9ec7de473e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ¤— AutoTrain LLM\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - upload train.csv to a folder named `data/`\n",
        "#@markdown - train.csv must contain a `text` column\n",
        "#@markdown - choose a project name if you wish\n",
        "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
        "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
        "#@markdown - update hyperparameters if you wish\n",
        "#@markdown - click `Runtime > Run all` or run each cell individually\n",
        "#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n",
        "\n",
        "import os\n",
        "!pip install -U autotrain-advanced > install_logs.txt\n",
        "!autotrain setup --colab > setup_logs.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2-_lkBS1WKA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#@markdown ---\n",
        "#@markdown #### Project Config\n",
        "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
        "project_name = 'lunar-llm-Llama-2-7B' # @param {type:\"string\"}\n",
        "model_name = 'TinyPixel/Llama-2-7B-bf16-sharded' # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Push to Hub?\n",
        "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
        "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
        "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
        "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
        "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "hf_token = \"hf_secret\" #@param {type:\"string\"}\n",
        "repo_id = \"404NotF0und/lunar-llm-Llama-2-7B\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Hyperparameters\n",
        "learning_rate = 2e-4 # @param {type:\"number\"}\n",
        "num_epochs = 1 #@param {type:\"number\"}\n",
        "batch_size = 5 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "block_size = 2048 # @param {type:\"number\"}\n",
        "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
        "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "gradient_accumulation = 4 # @param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n",
        "peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\n",
        "lora_r = 64 #@param {type:\"number\"}\n",
        "lora_alpha = 16 #@param {type:\"number\"}\n",
        "lora_dropout = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "os.environ[\"PROJECT_NAME\"] = project_name\n",
        "os.environ[\"MODEL_NAME\"] = model_name\n",
        "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"REPO_ID\"] = repo_id\n",
        "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
        "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
        "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
        "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
        "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
        "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
        "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
        "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
        "os.environ[\"PEFT\"] = str(peft)\n",
        "os.environ[\"QUANTIZATION\"] = str(quantization)\n",
        "os.environ[\"LORA_R\"] = str(lora_r)\n",
        "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
        "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/404NotF0und/MtG-json-to-ForgeScribe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvX5yhjB7_aP",
        "outputId": "66ebdb4d-addb-412b-87ed-6c8cf4a25783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MtG-json-to-ForgeScribe'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (16/16), 572.59 KiB | 2.54 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3cd_ED_yXXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2352e830-3042-40e4-ca28-b7b03aaf09c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "> \u001b[1mINFO    Running LLM\u001b[0m\n",
            "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.01, max_grad_norm=1.0, add_eos_token=False, block_size=1024, peft=True, lora_r=64, lora_alpha=16, lora_dropout=0.1, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision='fp16', quantization='int4', model_max_length=1024, trainer='default', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, apply_chat_template=False, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token='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', repo_id='404NotF0und/lunar-llm-Llama-2-7B', push_to_hub=True, model='TinyPixel/Llama-2-7B-bf16-sharded', project_name='lunar-llm-Llama-2-7B', seed=42, epochs=1, gradient_accumulation=4, disable_gradient_checkpointing=False, lr=0.0002, log='none', data_path='/content/MtG-json-to-ForgeScribe/', train_split='train', valid_split=None, batch_size=5, func=<function run_llm_command_factory at 0x7a6985ebdb40>)\u001b[0m\n",
            "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
            "> \u001b[1mINFO    {\"model\":\"TinyPixel/Llama-2-7B-bf16-sharded\",\"project_name\":\"lunar-llm-Llama-2-7B\",\"data_path\":\"/content/MtG-json-to-ForgeScribe/\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":1024,\"model_max_length\":1024,\"padding\":null,\"trainer\":\"default\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":\"fp16\",\"lr\":0.0002,\"epochs\":1,\"batch_size\":5,\"warmup_ratio\":0.1,\"gradient_accumulation\":4,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.01,\"max_grad_norm\":1.0,\"seed\":42,\"apply_chat_template\":false,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":64,\"lora_alpha\":16,\"lora_dropout\":0.1,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"prompt\",\"text_column\":\"text\",\"rejected_text_column\":\"rejected\",\"push_to_hub\":true,\"repo_id\":\"404NotF0und/lunar-llm-Llama-2-7B\",\"username\":null,\"token\":\"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\"}\u001b[0m\n",
            "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'lunar-llm-Llama-2-7B/training_params.json']\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-29 17:26:00\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['instruction', 'input', 'output', 'text'],\n",
            "    num_rows: 19100\n",
            "})\u001b[0m\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-29 17:26:00\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "Loading checkpoint shards: 100% 14/14 [01:12<00:00,  5.19s/it]\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-29 17:27:14\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
            "Running tokenizer on train dataset:   0% 0/19100 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1220 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on train dataset: 100% 19100/19100 [00:14<00:00, 1360.91 examples/s]\n",
            "Grouping texts in chunks of 1024 (num_proc=4): 100% 19100/19100 [00:09<00:00, 1950.24 examples/s]\n",
            "\u001b[1mðŸš€ INFO  \u001b[0m | \u001b[32m2024-01-29 17:27:38\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            " 54% 185/340 [3:24:34<2:51:23, 66.34s/it]"
          ]
        }
      ],
      "source": [
        "!autotrain llm \\\n",
        "--train \\\n",
        "--model ${MODEL_NAME} \\\n",
        "--project-name ${PROJECT_NAME} \\\n",
        "--data-path /content/MtG-json-to-ForgeScribe/ \\\n",
        "--text-column text \\\n",
        "--lr ${LEARNING_RATE} \\\n",
        "--batch-size ${BATCH_SIZE} \\\n",
        "--epochs ${NUM_EPOCHS} \\\n",
        "--block-size ${BLOCK_SIZE} \\\n",
        "--warmup-ratio ${WARMUP_RATIO} \\\n",
        "--lora-r ${LORA_R} \\\n",
        "--lora-alpha ${LORA_ALPHA} \\\n",
        "--lora-dropout ${LORA_DROPOUT} \\\n",
        "--weight-decay ${WEIGHT_DECAY} \\\n",
        "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
        "--quantization ${QUANTIZATION} \\\n",
        "--mixed-precision ${MIXED_PRECISION} \\\n",
        "$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n",
        "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}