{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZarakiKanzaki/project-lunar-ML/blob/main/lunar-llm-llama2-7b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ruEkBUtuEzGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "JvMRbVLEJlZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf402ec3-3f25-441b-e491-57f38d5b52b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m> \u001b[1mINFO    Installing latest xformers\u001b[0m\n",
            "> \u001b[1mINFO    Successfully installed latest xformers\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title 🤗 AutoTrain LLM\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - upload train.csv to a folder named `data/`\n",
        "#@markdown - train.csv must contain a `text` column\n",
        "#@markdown - choose a project name if you wish\n",
        "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
        "#@markdown - add huggingface information (token and repo_id) if you wish to push trained model to huggingface hub\n",
        "#@markdown - update hyperparameters if you wish\n",
        "#@markdown - click `Runtime > Run all` or run each cell individually\n",
        "#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n",
        "\n",
        "import os\n",
        "!pip install -U autotrain-advanced > install_logs.txt\n",
        "!autotrain setup --colab > setup_logs.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A2-_lkBS1WKA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#@markdown ---\n",
        "#@markdown #### Project Config\n",
        "#@markdown Note: if you are using a restricted/private model, you need to enter your Hugging Face token in the next step.\n",
        "project_name = 'lunar-llm-Llama-2-7B' # @param {type:\"string\"}\n",
        "model_name = 'TinyPixel/Llama-2-7B-bf16-sharded' # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Push to Hub?\n",
        "#@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account\n",
        "#@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.\n",
        "#@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.\n",
        "#@markdown You can find your token here: https://huggingface.co/settings/tokens\n",
        "push_to_hub = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "hf_token = \"hf_\" #@param {type:\"string\"}\n",
        "repo_id = \"404NotF0und/lunar-llm-Llama-2-7B\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Hyperparameters\n",
        "learning_rate = 2e-4 # @param {type:\"number\"}\n",
        "num_epochs = 1 #@param {type:\"number\"}\n",
        "batch_size = 4 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "block_size = 2048 # @param {type:\"number\"}\n",
        "trainer = \"sft\" # @param [\"default\", \"sft\"] {type:\"raw\"}\n",
        "warmup_ratio = 0.1 # @param {type:\"number\"}\n",
        "weight_decay = 0.01 # @param {type:\"number\"}\n",
        "gradient_accumulation = 4 # @param {type:\"number\"}\n",
        "mixed_precision = \"fp16\" # @param [\"fp16\", \"bf16\", \"none\"] {type:\"raw\"}\n",
        "peft = True # @param [\"False\", \"True\"] {type:\"raw\"}\n",
        "quantization = \"int4\" # @param [\"int4\", \"int8\", \"none\"] {type:\"raw\"}\n",
        "lora_r = 64 #@param {type:\"number\"}\n",
        "lora_alpha = 16 #@param {type:\"number\"}\n",
        "lora_dropout = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "os.environ[\"PROJECT_NAME\"] = project_name\n",
        "os.environ[\"MODEL_NAME\"] = model_name\n",
        "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "os.environ[\"REPO_ID\"] = repo_id\n",
        "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
        "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
        "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
        "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
        "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
        "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
        "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
        "os.environ[\"MIXED_PRECISION\"] = str(mixed_precision)\n",
        "os.environ[\"PEFT\"] = str(peft)\n",
        "os.environ[\"QUANTIZATION\"] = str(quantization)\n",
        "os.environ[\"LORA_R\"] = str(lora_r)\n",
        "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
        "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/404NotF0und/MtG-json-to-ForgeScribe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvX5yhjB7_aP",
        "outputId": "932a5e71-fdba-43a5-9d89-b474046f1e12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MtG-json-to-ForgeScribe'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 16 (delta 2), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (16/16), 572.59 KiB | 5.25 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g3cd_ED_yXXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081dde9f-e80b-4309-df16-854ed26589f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "> \u001b[1mINFO    Running LLM\u001b[0m\n",
            "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.01, max_grad_norm=1.0, add_eos_token=False, block_size=2048, peft=True, lora_r=64, lora_alpha=16, lora_dropout=0.1, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision='fp16', quantization='int4', model_max_length=1024, trainer='default', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, apply_chat_template=False, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token='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', repo_id='404NotF0und/lunar-llm-Llama-2-7B', push_to_hub=True, model='TinyPixel/Llama-2-7B-bf16-sharded', project_name='lunar-llm-Llama-2-7B', seed=42, epochs=1, gradient_accumulation=4, disable_gradient_checkpointing=False, lr=0.0002, log='none', data_path='/content/MtG-json-to-ForgeScribe/', train_split='train', valid_split=None, batch_size=6, func=<function run_llm_command_factory at 0x7c1f8eec5d80>)\u001b[0m\n",
            "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
            "> \u001b[1mINFO    {\"model\":\"TinyPixel/Llama-2-7B-bf16-sharded\",\"project_name\":\"lunar-llm-Llama-2-7B\",\"data_path\":\"/content/MtG-json-to-ForgeScribe/\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":2048,\"model_max_length\":1024,\"padding\":null,\"trainer\":\"default\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":\"fp16\",\"lr\":0.0002,\"epochs\":1,\"batch_size\":6,\"warmup_ratio\":0.1,\"gradient_accumulation\":4,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.01,\"max_grad_norm\":1.0,\"seed\":42,\"apply_chat_template\":false,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":64,\"lora_alpha\":16,\"lora_dropout\":0.1,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"prompt\",\"text_column\":\"text\",\"rejected_text_column\":\"rejected\",\"push_to_hub\":true,\"repo_id\":\"404NotF0und/lunar-llm-Llama-2-7B\",\"username\":null,\"token\":\"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\"}\u001b[0m\n",
            "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'lunar-llm-Llama-2-7B/training_params.json']\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-01-30 16:17:32\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mTrain data: Dataset({\n",
            "    features: ['instruction', 'input', 'output', 'text'],\n",
            "    num_rows: 19100\n",
            "})\u001b[0m\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-01-30 16:17:32\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
            "Loading checkpoint shards: 100% 14/14 [00:18<00:00,  1.31s/it]\n",
            "\u001b[33m\u001b[1m⚠️ WARNING\u001b[0m | \u001b[32m2024-01-30 16:17:52\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m270\u001b[0m - \u001b[33m\u001b[1mThe block_size passed (2048) is larger than the maximum length for the model(1024). Using block_size=1024.\u001b[0m\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-01-30 16:17:52\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m277\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
            "Running tokenizer on train dataset:   0% 0/19100 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1220 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on train dataset: 100% 19100/19100 [00:04<00:00, 3986.75 examples/s]\n",
            "Grouping texts in chunks of 1024 (num_proc=4): 100% 19100/19100 [00:02<00:00, 6734.43 examples/s]\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-01-30 16:18:00\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m339\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
            "{'loss': 0.6296, 'learning_rate': 4.470588235294118e-05, 'epoch': 0.8}\n",
            "100% 284/284 [1:47:20<00:00, 21.37s/it]Checkpoint destination directory lunar-llm-Llama-2-7B/checkpoint-284 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
            "{'train_runtime': 6441.5827, 'train_samples_per_second': 1.057, 'train_steps_per_second': 0.044, 'train_loss': 0.5844033268136037, 'epoch': 1.0}\n",
            "100% 284/284 [1:47:21<00:00, 22.68s/it]\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-01-30 18:05:21\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m477\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n",
            "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-01-30 18:05:22\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m510\u001b[0m - \u001b[1mPushing model to hub...\u001b[0m\n",
            "adapter_model.safetensors:   0% 0.00/134M [00:00<?, ?B/s]\n",
            "adapter_model.safetensors:   0% 0.00/134M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "optimizer.pt:   0% 0.00/269M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin:   0% 0.00/443 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 10 LFS files:   0% 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth:   0% 0.00/14.6k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:   0% 16.4k/269M [00:00<41:16, 108kB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch_model.bin: 100% 443/443 [00:00<00:00, 2.11kB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:   0% 16.4k/134M [00:00<30:21, 73.7kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rng_state.pth: 100% 14.6k/14.6k [00:00<00:00, 68.9kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "adapter_model.safetensors:   2% 2.65M/134M [00:00<00:13, 9.87MB/s]\n",
            "adapter_model.safetensors:   4% 4.92M/134M [00:00<00:08, 14.9MB/s]\u001b[A\n",
            "\n",
            "adapter_model.safetensors:   3% 4.46M/134M [00:00<00:12, 10.4MB/s]\n",
            "\n",
            "optimizer.pt:   3% 7.83M/269M [00:00<00:15, 16.8MB/s]\u001b[A\u001b[A\n",
            "rng_state.pth: 100% 14.6k/14.6k [00:00<00:00, 25.2kB/s]\n",
            "pytorch_model.bin: 100% 443/443 [00:00<00:00, 738B/s]  \n",
            "adapter_model.safetensors:   6% 7.70M/134M [00:00<00:08, 15.0MB/s]\n",
            "adapter_model.safetensors:   8% 11.3M/134M [00:00<00:06, 19.2MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:   4% 10.4M/269M [00:00<00:18, 13.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   9% 11.7M/134M [00:00<00:08, 14.2MB/s]\n",
            "\n",
            "optimizer.pt:   5% 13.6M/269M [00:00<00:16, 15.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "scheduler.pt: 100% 627/627 [00:00<00:00, 3.13kB/s]\n",
            "\n",
            "adapter_model.safetensors:  12% 16.0M/134M [00:01<00:08, 13.7MB/s]\u001b[A\n",
            "adapter_model.safetensors:  16% 21.8M/134M [00:01<00:05, 19.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "training_args.bin:   0% 0.00/4.28k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  12% 16.0M/134M [00:01<00:10, 11.0MB/s]\n",
            "adapter_model.safetensors:  20% 26.6M/134M [00:01<00:04, 23.6MB/s]\n",
            "\n",
            "optimizer.pt:   8% 21.9M/269M [00:01<00:15, 16.2MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  21% 28.1M/134M [00:01<00:05, 18.3MB/s]\u001b[A\n",
            "\n",
            "training_args.bin: 100% 4.28k/4.28k [00:00<00:00, 10.3kB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 621kB/s]\n",
            "adapter_model.safetensors:  24% 32.0M/134M [00:01<00:04, 20.7MB/s]\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  27% 36.6M/134M [00:01<00:04, 24.4MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "training_args.bin:   0% 0.00/4.28k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  24% 32.0M/134M [00:02<00:06, 14.8MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  11% 28.5M/269M [00:02<00:19, 12.6MB/s]\u001b[A\u001b[A\n",
            "training_args.bin: 100% 4.28k/4.28k [00:00<00:00, 10.9kB/s]\n",
            "\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 839kB/s]\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  34% 45.5M/134M [00:02<00:05, 17.2MB/s]\n",
            "adapter_model.safetensors:  39% 52.3M/134M [00:02<00:03, 24.7MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  14% 38.1M/269M [00:02<00:14, 15.8MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  42% 55.7M/134M [00:02<00:03, 24.0MB/s]\u001b[A\n",
            "adapter_model.safetensors:  36% 48.1M/134M [00:03<00:07, 11.5MB/s]\n",
            "\n",
            "optimizer.pt:  16% 42.2M/269M [00:03<00:17, 12.6MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  46% 61.2M/134M [00:03<00:02, 24.4MB/s]\n",
            "\n",
            "optimizer.pt:  16% 44.2M/269M [00:03<00:17, 13.0MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  50% 66.5M/134M [00:03<00:03, 20.8MB/s]\n",
            "adapter_model.safetensors:  53% 71.2M/134M [00:03<00:02, 23.0MB/s]\n",
            "adapter_model.safetensors:  58% 77.3M/134M [00:03<00:02, 28.2MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  18% 48.0M/269M [00:03<00:22, 9.92MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  61% 81.9M/134M [00:04<00:02, 25.1MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  20% 53.8M/269M [00:04<00:15, 13.9MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  69% 92.5M/134M [00:04<00:01, 38.2MB/s]\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  58% 78.1M/134M [00:04<00:03, 16.4MB/s]\n",
            "adapter_model.safetensors:  73% 98.0M/134M [00:04<00:01, 29.3MB/s]\u001b[A\n",
            "adapter_model.safetensors:  82% 110M/134M [00:04<00:00, 43.3MB/s] \u001b[A\n",
            "\n",
            "adapter_model.safetensors:  60% 80.7M/134M [00:04<00:04, 12.0MB/s]\n",
            "adapter_model.safetensors:  64% 85.9M/134M [00:04<00:02, 16.6MB/s]\n",
            "\n",
            "optimizer.pt:  24% 64.0M/269M [00:05<00:19, 10.3MB/s]\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  90% 121M/134M [00:05<00:00, 28.6MB/s]\u001b[A\n",
            "\n",
            "optimizer.pt:  28% 76.5M/269M [00:05<00:08, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  67% 90.4M/134M [00:05<00:03, 12.8MB/s]\n",
            "adapter_model.safetensors:  93% 125M/134M [00:05<00:00, 20.9MB/s]\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  70% 93.6M/134M [00:05<00:02, 13.9MB/s]\n",
            "\n",
            "optimizer.pt:  36% 97.9M/269M [00:05<00:06, 28.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  72% 96.0M/134M [00:06<00:03, 10.6MB/s]\n",
            "adapter_model.safetensors: 100% 134M/134M [00:06<00:00, 20.1MB/s]\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  82% 110M/134M [00:06<00:01, 14.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Upload 10 LFS files:  10% 1/10 [00:07<01:03,  7.04s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  42% 113M/269M [00:07<00:11, 14.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  95% 127M/134M [00:07<00:00, 27.3MB/s]\n",
            "\n",
            "adapter_model.safetensors: 100% 134M/134M [00:07<00:00, 16.8MB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:  54% 144M/269M [00:07<00:05, 24.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  59% 160M/269M [00:08<00:02, 38.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  62% 167M/269M [00:08<00:03, 32.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 10 LFS files:  20% 2/10 [00:08<00:30,  3.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  66% 176M/269M [00:08<00:03, 30.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  71% 190M/269M [00:08<00:01, 44.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  74% 198M/269M [00:09<00:01, 38.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  77% 208M/269M [00:09<00:01, 35.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  83% 222M/269M [00:09<00:00, 49.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  86% 230M/269M [00:09<00:01, 38.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  89% 240M/269M [00:10<00:00, 32.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  95% 254M/269M [00:10<00:00, 46.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt: 100% 269M/269M [00:11<00:00, 23.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Upload 10 LFS files: 100% 10/10 [00:11<00:00,  1.16s/it]\n"
          ]
        }
      ],
      "source": [
        "!autotrain llm \\\n",
        "--train \\\n",
        "--model ${MODEL_NAME} \\\n",
        "--project-name ${PROJECT_NAME} \\\n",
        "--data-path /content/MtG-json-to-ForgeScribe/ \\\n",
        "--text-column text \\\n",
        "--lr ${LEARNING_RATE} \\\n",
        "--batch-size ${BATCH_SIZE} \\\n",
        "--epochs ${NUM_EPOCHS} \\\n",
        "--block-size ${BLOCK_SIZE} \\\n",
        "--warmup-ratio ${WARMUP_RATIO} \\\n",
        "--lora-r ${LORA_R} \\\n",
        "--lora-alpha ${LORA_ALPHA} \\\n",
        "--lora-dropout ${LORA_DROPOUT} \\\n",
        "--weight-decay ${WEIGHT_DECAY} \\\n",
        "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
        "--quantization ${QUANTIZATION} \\\n",
        "--mixed-precision ${MIXED_PRECISION} \\\n",
        "$( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n",
        "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying the result"
      ],
      "metadata": {
        "id": "3q7pbe3pI6eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the contents of the latest uploaded file\n",
        "latest_file_path = '/mnt/data/logllm.log'\n",
        "\n",
        "with open(latest_file_path, 'r') as file:\n",
        "    latest_log_data = file.readlines()\n",
        "\n",
        "# Convert the string representations of dictionaries to actual dictionaries\n",
        "latest_log_dicts = [ast.literal_eval(line.strip()) for line in latest_log_data]\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "latest_df = pd.DataFrame(latest_log_dicts)\n",
        "\n",
        "# Filtering the DataFrame to include data from 0.06 epochs onwards\n",
        "latest_filtered_df = latest_df[latest_df['epoch'] >= 0.06]\n",
        "\n",
        "# Fit the polynomial regression model to the latest data\n",
        "model.fit(latest_filtered_df[['epoch']], latest_filtered_df['loss'])\n",
        "\n",
        "# Generating latest predictions for the trend line\n",
        "latest_trend_line = model.predict(latest_filtered_df[['epoch']])\n",
        "\n",
        "# Re-plotting with the latest filtered data and the polynomial trend line\n",
        "\n",
        "# Setting up the figure and axis for the plot\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Plotting Loss\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss', color='tab:red')\n",
        "ax1.plot(latest_filtered_df['epoch'], latest_filtered_df['loss'], color='tab:red', label='Loss')\n",
        "ax1.plot(latest_filtered_df['epoch'], latest_trend_line, color='tab:green', label='Trend Line (Loss) - Polynomial')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# Creating a second y-axis to plot Learning Rate\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Learning Rate', color='tab:blue')\n",
        "ax2.plot(latest_filtered_df['epoch'], latest_filtered_df['learning_rate'], color='tab:blue', label='Learning Rate')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Title and layout adjustments\n",
        "plt.title('Latest Loss and Learning Rate with Polynomial Trend Line from 0.06 Epochs Onwards')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Displaying the updated plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_hdX1TjgI5_Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}